{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7f468-79e9-48e6-9e6f-51dac6116fb0",
   "metadata": {},
   "source": [
    "# install requirements\n",
    "!pip install pinecone-client openai burr[graphviz] "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5a2c326f-53cc-4eb1-8508-e06584d8051c",
   "metadata": {},
   "source": [
    "# Question & answer notebook\n",
    "\n",
    "This notebook walks you through how to build a burr application that talks to pinecone and openai to answer questions about UFC fights."
   ]
  },
  {
   "cell_type": "code",
   "id": "39233706-20fd-4043-bc0f-291626664f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:39:04.466705Z",
     "start_time": "2024-05-25T20:39:03.822695Z"
    }
   },
   "source": [
    "# import what we need\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import openai\n",
    "from burr.core import ApplicationBuilder, State, default, expr, Application\n",
    "from burr.core.action import action\n",
    "from burr.tracking import LocalTrackingClient\n",
    "import uuid\n",
    "import pinecone"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a2fe5f83-75b4-4d44-a80e-829947b6d240",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "We first set up some helper functions that we'll use."
   ]
  },
  {
   "cell_type": "code",
   "id": "ef809095-566a-40af-a8a6-cbf72ef21227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:39:03.820541Z",
     "start_time": "2024-05-25T20:39:03.811750Z"
    }
   },
   "source": [
    "def set_inital_chat_history() -> list[dict]:\n",
    "    SYSTEM_MESSAGE = (\"You're a helpful assistance, base your answer only on the provided context, \"\n",
    "                      \"do not use preexisting knowledge.\")\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
    "    return messages\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Actions\n",
    "Let's now define the actions that our application will make and what they read from & write to with respect to state."
   ],
   "id": "f9332f928569da3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:39:47.581716Z",
     "start_time": "2024-05-25T20:39:47.577258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@action(\n",
    "    reads=[],\n",
    "    writes=[\"question\", \"chat_history\"],\n",
    ")\n",
    "def human_converse(state: State, user_question: str) -> Tuple[dict, State]:\n",
    "    \"\"\"Human converse step -- make sure we get input, and store it as state.\"\"\"\n",
    "    new_state = state.update(question=user_question)\n",
    "    new_state = new_state.append(chat_history={\"role\": \"user\", \"content\": user_question})\n",
    "    return {\"question\": user_question}, new_state\n"
   ],
   "id": "96c10e1f42784fad",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:39:59.443938Z",
     "start_time": "2024-05-25T20:39:59.439885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@action(\n",
    "    reads=[\"question\", \"chat_history\"],\n",
    "    writes=[\"chat_history\", \"vector\"],\n",
    ")\n",
    "def extract_embedding(state: State, client: openai.Client) -> tuple[dict, State]:\n",
    "    \"\"\"AI step to create the text embedding.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    # Create embeddings from user question\n",
    "    response = client.embeddings.create(\n",
    "        input=question,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    vector = response.data[0].embedding\n",
    "    new_state = state.update(vector=vector)\n",
    "    return {\"usage\": response.usage.to_dict()}, new_state"
   ],
   "id": "fd50b7ec91fa01a7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:40:18.270475Z",
     "start_time": "2024-05-25T20:40:18.265421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@action(\n",
    "    reads=[\"vector\", \"chat_history\", \"question\"],\n",
    "    writes=[\"vector\", \"chat_history\"],\n",
    ")\n",
    "def query_vectordb(state: State, vector_db_index: pinecone.Index, top_k: int = 20) -> Tuple[dict, State]:\n",
    "    \"\"\"Query the vector DB to create the message context.\"\"\"\n",
    "    vector = state[\"vector\"]\n",
    "    response = vector_db_index.query(vector=vector,\n",
    "                                     top_k=top_k,\n",
    "                                     include_values=False,\n",
    "                                     includeMetadata=True)\n",
    "    matches = response['matches']\n",
    "    context = []\n",
    "    for match in matches:\n",
    "        context.append(match['metadata'])\n",
    "    content = f\"Please use this context: {context}\\n To answer the following question: {question}.\"\n",
    "    new_state = state.append(chat_history={\"role\": \"user\", \"content\": content})\n",
    "    new_state = new_state.update(vector=None)\n",
    "    return {\"num_results\": len(response['matches'])}, new_state"
   ],
   "id": "2052e20b9c7fcc53",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:40:23.517860Z",
     "start_time": "2024-05-25T20:40:23.513910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@action(\n",
    "    reads=[\"chat_history\"],\n",
    "    writes=[\"chat_history\"],\n",
    ")\n",
    "def AI_generate_response(state: State, client: openai.Client) -> tuple[dict, State]:\n",
    "    \"\"\"AI step to generate the response.\"\"\"\n",
    "    messages = state[\"chat_history\"]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=messages,\n",
    "    )  # get a new response from the model where it can see the function response\n",
    "    response_message = response.choices[0].message\n",
    "    new_state = state.append(chat_history=response_message.to_dict())\n",
    "    return {\"ai_response\": response_message.content,\n",
    "            \"usage\": response.usage.to_dict()}, new_state"
   ],
   "id": "6cb12caa87259690",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define the application\n",
    "This is where we define our application now"
   ],
   "id": "b05082f98ef7aa50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:42:35.073418Z",
     "start_time": "2024-05-25T20:42:35.057783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define our clients / connections / IDs\n",
    "openai_client = openai.OpenAI()\n",
    "db_client = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"ufc\"\n",
    "index = db_client.Index(index_name)\n",
    "application_run_id = str(uuid.uuid4())"
   ],
   "id": "afcc4b7f912e0fcd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:42:36.006779Z",
     "start_time": "2024-05-25T20:42:36.003617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set the initial chat history\n",
    "base_messages = set_inital_chat_history()"
   ],
   "id": "8946418f98ee042e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:42:36.457293Z",
     "start_time": "2024-05-25T20:42:36.448126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tracker = LocalTrackingClient(\"ufc-pinecone\")\n",
    "# create graph\n",
    "burr_application = (\n",
    "    ApplicationBuilder()\n",
    "    .with_actions(  # define the actions\n",
    "        extract_embedding.bind(client=openai_client),\n",
    "        query_vectordb.bind(vector_db_index=index),\n",
    "        AI_generate_response.bind(client=openai_client),\n",
    "        human_converse\n",
    "    )\n",
    "    .with_transitions(  # define the edges between the actions based on state conditions\n",
    "        (\"human_converse\", \"extract_embedding\", default),\n",
    "        (\"extract_embedding\", \"query_vectordb\", default),\n",
    "        (\"query_vectordb\", \"AI_generate_response\", default),\n",
    "        (\"AI_generate_response\", \"human_converse\", default)\n",
    "    )\n",
    "    .with_identifiers(app_id=application_run_id)\n",
    "    .with_state(  # initial state\n",
    "        **{\"chat_history\": base_messages, \"tool_calls\": []},\n",
    "    )\n",
    "    .with_entrypoint(\"human_converse\")\n",
    "    .with_tracker(tracker)\n",
    "    .build()\n",
    ")"
   ],
   "id": "8c5d18cd1b63afc9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T20:42:42.601153Z",
     "start_time": "2024-05-25T20:42:42.290426Z"
    }
   },
   "cell_type": "code",
   "source": "burr_application.visualize(include_conditions=True)",
   "id": "231cf366a7db4024",
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n -->\n<!-- Pages: 1 -->\n<svg width=\"398pt\" height=\"239pt\"\n viewBox=\"0.00 0.00 398.11 239.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 235)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-235 394.11,-235 394.11,4 -4,4\"/>\n<!-- extract_embedding -->\n<g id=\"node1\" class=\"node\">\n<title>extract_embedding</title>\n<path fill=\"none\" stroke=\"black\" d=\"M238.08,-231C238.08,-231 141.83,-231 141.83,-231 135.83,-231 129.83,-225 129.83,-219 129.83,-219 129.83,-207 129.83,-207 129.83,-201 135.83,-195 141.83,-195 141.83,-195 238.08,-195 238.08,-195 244.08,-195 250.08,-201 250.08,-207 250.08,-207 250.08,-219 250.08,-219 250.08,-225 244.08,-231 238.08,-231\"/>\n<text text-anchor=\"middle\" x=\"189.96\" y=\"-207.95\" font-family=\"Times,serif\" font-size=\"14.00\">extract_embedding</text>\n</g>\n<!-- query_vectordb -->\n<g id=\"node2\" class=\"node\">\n<title>query_vectordb</title>\n<path fill=\"none\" stroke=\"black\" d=\"M132.33,-166C132.33,-166 55.58,-166 55.58,-166 49.58,-166 43.58,-160 43.58,-154 43.58,-154 43.58,-142 43.58,-142 43.58,-136 49.58,-130 55.58,-130 55.58,-130 132.33,-130 132.33,-130 138.33,-130 144.33,-136 144.33,-142 144.33,-142 144.33,-154 144.33,-154 144.33,-160 138.33,-166 132.33,-166\"/>\n<text text-anchor=\"middle\" x=\"93.96\" y=\"-142.95\" font-family=\"Times,serif\" font-size=\"14.00\">query_vectordb</text>\n</g>\n<!-- extract_embedding&#45;&gt;query_vectordb -->\n<g id=\"edge4\" class=\"edge\">\n<title>extract_embedding&#45;&gt;query_vectordb</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.47,-194.62C153.08,-187.8 141,-179.87 129.85,-172.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.9,-169.72 121.62,-167.16 128.06,-175.57 131.9,-169.72\"/>\n</g>\n<!-- AI_generate_response -->\n<g id=\"node4\" class=\"node\">\n<title>AI_generate_response</title>\n<path fill=\"none\" stroke=\"black\" d=\"M150.33,-101C150.33,-101 37.58,-101 37.58,-101 31.58,-101 25.58,-95 25.58,-89 25.58,-89 25.58,-77 25.58,-77 25.58,-71 31.58,-65 37.58,-65 37.58,-65 150.33,-65 150.33,-65 156.33,-65 162.33,-71 162.33,-77 162.33,-77 162.33,-89 162.33,-89 162.33,-95 156.33,-101 150.33,-101\"/>\n<text text-anchor=\"middle\" x=\"93.96\" y=\"-77.95\" font-family=\"Times,serif\" font-size=\"14.00\">AI_generate_response</text>\n</g>\n<!-- query_vectordb&#45;&gt;AI_generate_response -->\n<g id=\"edge5\" class=\"edge\">\n<title>query_vectordb&#45;&gt;AI_generate_response</title>\n<path fill=\"none\" stroke=\"black\" d=\"M93.96,-129.78C93.96,-124.37 93.96,-118.24 93.96,-112.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"97.46,-112.71 93.96,-102.71 90.46,-112.71 97.46,-112.71\"/>\n</g>\n<!-- input__top_k -->\n<g id=\"node3\" class=\"node\">\n<title>input__top_k</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"55.96\" cy=\"-213\" rx=\"55.96\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"55.96\" y=\"-207.95\" font-family=\"Times,serif\" font-size=\"14.00\">input: top_k</text>\n</g>\n<!-- input__top_k&#45;&gt;query_vectordb -->\n<g id=\"edge1\" class=\"edge\">\n<title>input__top_k&#45;&gt;query_vectordb</title>\n<path fill=\"none\" stroke=\"black\" d=\"M66.14,-195.12C69.66,-189.28 73.7,-182.58 77.59,-176.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"80.53,-178.04 82.7,-167.66 74.54,-174.42 80.53,-178.04\"/>\n</g>\n<!-- human_converse -->\n<g id=\"node5\" class=\"node\">\n<title>human_converse</title>\n<path fill=\"none\" stroke=\"black\" d=\"M232.08,-36C232.08,-36 147.83,-36 147.83,-36 141.83,-36 135.83,-30 135.83,-24 135.83,-24 135.83,-12 135.83,-12 135.83,-6 141.83,0 147.83,0 147.83,0 232.08,0 232.08,0 238.08,0 244.08,-6 244.08,-12 244.08,-12 244.08,-24 244.08,-24 244.08,-30 238.08,-36 232.08,-36\"/>\n<text text-anchor=\"middle\" x=\"189.96\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">human_converse</text>\n</g>\n<!-- AI_generate_response&#45;&gt;human_converse -->\n<g id=\"edge6\" class=\"edge\">\n<title>AI_generate_response&#45;&gt;human_converse</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.44,-64.62C130.83,-57.8 142.91,-49.87 154.07,-42.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"155.85,-45.57 162.29,-37.16 152.01,-39.72 155.85,-45.57\"/>\n</g>\n<!-- human_converse&#45;&gt;extract_embedding -->\n<g id=\"edge3\" class=\"edge\">\n<title>human_converse&#45;&gt;extract_embedding</title>\n<path fill=\"none\" stroke=\"black\" d=\"M189.96,-36.29C189.96,-69.52 189.96,-142.61 189.96,-183.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.46,-183.02 189.96,-193.02 193.46,-183.02 186.46,-183.02\"/>\n</g>\n<!-- input__user_question -->\n<g id=\"node6\" class=\"node\">\n<title>input__user_question</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"303.96\" cy=\"-83\" rx=\"86.15\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"303.96\" y=\"-77.95\" font-family=\"Times,serif\" font-size=\"14.00\">input: user_question</text>\n</g>\n<!-- input__user_question&#45;&gt;human_converse -->\n<g id=\"edge2\" class=\"edge\">\n<title>input__user_question&#45;&gt;human_converse</title>\n<path fill=\"none\" stroke=\"black\" d=\"M274.6,-65.78C261.54,-58.56 245.93,-49.93 231.69,-42.06\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"233.39,-39 222.94,-37.23 230,-45.13 233.39,-39\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x111465960>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the application\n",
    "Here we show how to do a simple loop stopping before `human_converse` each time to get user input before running the graph again.\n",
    "\n",
    "\n",
    "### Viewing a trace of the this application in the Burr UI\n",
    "Note: you can view the logs of the conversation in the Burr UI. \n",
    "\n",
    "To see that, in another terminal do:\n",
    "\n",
    "> burr\n",
    "\n",
    "You'll then have the UI running on [http://localhost:7241/](http://localhost:7241/).\n",
    "\n",
    "#### Using the Burr UI in google collab\n",
    "To use the UI in google collab do the following:\n",
    "\n",
    "1. Run this in a cell\n",
    "```python\n",
    "from google.colab import output\n",
    "output.serve_kernel_port_as_window(7241)\n",
    "```\n",
    "\n",
    "2. Then start the burr UI:\n",
    "```\n",
    "!burr &\n",
    "```\n",
    "3. Click the link in (1) to open a new tab."
   ],
   "id": "4fcf7679726c717a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run it\n",
    "while True:\n",
    "    # this will ask for input:\n",
    "    question = input(\"What can I help you with?\\n\")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "    current_action, _, current_state = burr_application.run(\n",
    "        halt_before=[\"human_converse\"],\n",
    "        inputs={\"user_question\": question},\n",
    "    )\n",
    "    # we'll then see the AI response:\n",
    "    print(f\"AI: {current_state['chat_history'][-1]['content']}\\n\")\n",
    "current_state"
   ],
   "id": "7b13c134ad8c5c99"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
